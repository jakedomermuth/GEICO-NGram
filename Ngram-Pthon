import nltk
nltk.download('punkt')
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk import ngrams
from collections import Counter


def preprocess(text):
    text = text.lower()  
    text = ''.join([char for char in text if char.isalpha() or char.isspace()]) 
    tokens = nltk.word_tokenize(text)  # Tokenize
    tokens = [token for token in tokens if token not in stopwords.words('english')]  
    return tokens


with open(r'C:\Users\jdome\PycharmProjects\pythonProject1\Geico_Reviews_for_Github.txt', 'r', encoding='utf-8') as file:
    reviews = file.readlines()


reviews = [preprocess(review.strip()) for review in reviews]


n = 4 #edit depending on desired lenght 
all_ngrams = []
for review in reviews:
    all_ngrams.extend(list(ngrams(review, n)))


ngram_counts = Counter(all_ngrams)


most_common_ngrams = ngram_counts.most_common(5) #Edit depending on desired number of results

print(most_common_ngrams)
